{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Scraping Project for Bandcamp by Domenick Avanzi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import csv \n",
    "artist_index_url = \"https://bandcamp.com/artist_index\" #Set the url for all artists\n",
    "html = urllib.request.urlopen(artist_index_url) # Open the page\n",
    "soup = BeautifulSoup(html,'html.parser') #Use beautifulSoup to access html  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = []\n",
    "# Finds artists on page\n",
    "for b in soup.find_all('a', title = True):\n",
    "    artists.append(b[\"title\"])\n",
    "    print(b[\"title\"])\n",
    "    \n",
    "print(\"number of artists: \", len(artists))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find total number of artists\n",
    "\n",
    "ARTIST_ = []\n",
    "ALBUM_ = []\n",
    "SONG_ = []\n",
    "\n",
    "with open('Test123.csv','w') as csvfile:\n",
    "    fieldnames = ['Artist','Album','Song']\n",
    "    writer = csv.DictWriter(csvfile,fieldnames=fieldnames,lineterminator = '\\n')\n",
    "    writer.writeheader()\n",
    "    \n",
    "    n_page = \"?page=1\"\n",
    "    for x in range(1,2):\n",
    "        n_page = n_page.replace(n_page[len(n_page) - 1], str(x)) # replace the previous page number    \n",
    "        page_html = artist_index_url + n_page # create the new link for the next page to scan\n",
    "        page_html = urllib.request.urlopen(page_html) # open the next page\n",
    "        soup = BeautifulSoup(page_html,'html.parser') # Pull the data from the next page\n",
    "        \n",
    "        # Get the artists on that page and append it to the complete artist list\n",
    "        for q in soup.find_all('a',title = True):\n",
    "            ARTIST_= q[\"title\"] #FIX THIS\n",
    "            print(ARTIST_)\n",
    "            z = soup.find('ul', {\"class\":\"item_list\"}) # Finds the html section that includes all artist links\n",
    "            for a_link in z.find_all('a', href = True):\n",
    "                print(a_link[\"href\"]) # print the artist's url\n",
    "                artist_url = a_link[\"href\"] # create variable for artist's url\n",
    "                artist_html = urllib.request.urlopen(artist_url) # open the artist's url\n",
    "                artist_soup = BeautifulSoup(artist_html,'html.parser') # Pull the data from artist's url\n",
    "            \n",
    "                try:\n",
    "                    # See if they have albums!\n",
    "                    \n",
    "                    album_soup = artist_soup.find('ol', {\"data-edit-callback\" : \"/music_reorder\"})\n",
    "                    for l in album_soup.find_all('a',href=True):\n",
    "                        print(l[\"href\"])\n",
    "                    \n",
    "                # If they don't have multiple albums just get the info on their page               \n",
    "                except: \n",
    "                    \n",
    "                    artist_name = artist_soup.find(\"span\",{\"itemprop\":\"byArtist\"})\n",
    "                    album_name = artist_soup.find(\"h2\",{\"class\":\"trackTitle\"})\n",
    "                    album_name = album_name.text\n",
    "                    album_name.replace(\"\\r\\n\",\"\")\n",
    "                    \n",
    "                    for song in artist_soup.find_all(\"span\", {\"itemprop\":\"name\"}):\n",
    "                        #print(song.text)\n",
    "                        SONG_.append(song.text)\n",
    "                n_songs = len(SONG_)\n",
    "                print(\"# Songs: \", n_songs)\n",
    "                \n",
    "                ALBUM_ = album_name\n",
    "                if n_songs > 0:\n",
    "                    for i in range(0,n_songs):\n",
    "                        print(i)\n",
    "                        print(ARTIST_)\n",
    "                        print(SONG_[i])\n",
    "                        myData = writer.writerow({'Artist':ARTIST_,'Album':ALBUM_,'Song':SONG_[i]})\n",
    "                        \n",
    "                SONG_= []\n",
    "                ARTIST_= []\n",
    "                ALBUM_= []\n",
    "                        \n",
    "                    \n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Suicide Silence\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = artist_soup.find(\"span\",{\"itemprop\":\"byArtist\"})\n",
    "print(m.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test bed for multipl albums\n",
    "artist_index_url = \"https://brotherjax.bandcamp.com\" #Set the url for all artists\n",
    "html = urllib.request.urlopen(artist_index_url) # Open the page\n",
    "artist_soup = BeautifulSoup(html,'html.parser') #Use beautifulSoup to access html \n",
    "\n",
    "try:\n",
    "    # See if they have albums!\n",
    "    multiple_albums = \"yes\"\n",
    "    album_soup = artist_soup.find('ol', {\"data-edit-callback\" : \"/music_reorder\"})\n",
    "    for l in album_soup.find_all('a',href=True):\n",
    "        print(l[\"href\"])\n",
    "except:\n",
    "    print('Only 1 album')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_albums = []\n",
    "for data_item in soup.find_all('li', {\"data-item-id\" : True}):\n",
    "    for album in \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol = soup.find('ol', {\"data-edit-callback\" : \"/music_reorder\"})\n",
    "\n",
    "for l in lol.find_all('a',href=True):\n",
    "    print(l[\"href\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "11\n",
      "<span class=\"time secondaryText\">\n",
      "        \n",
      "            05:35\n",
      "        \n",
      "        </span>\n",
      "<span class=\"time secondaryText\">\n",
      "        \n",
      "            03:32\n",
      "        \n",
      "        </span>\n",
      "<span class=\"time secondaryText\">\n",
      "        \n",
      "            04:12\n",
      "        \n",
      "        </span>\n",
      "<span class=\"time secondaryText\">\n",
      "        \n",
      "            01:43\n",
      "        \n",
      "        </span>\n",
      "['\\nAtariame\\n', '\\n            Voiceless\\n            \\n        ', 'Moscow, Russia', '20190621', ['ambient', 'idm', 'avant-pop', 'dark folk', 'rhythmic noise', 'shoegaze', 'Moscow'], ['Breath Exercise', 'Lost In A Forest', 'Mood Swings', 'Outside At 5 AM', 'Same Thought All Day', 'Green Trees Violet Sky', 'Desk Lamp', 'Stay Late', 'Split', 'Not Today', 'Deconstruction']]\n"
     ]
    }
   ],
   "source": [
    "TAGS_ = []\n",
    "SONG_ = []\n",
    "LENGTH_ = []\n",
    "\n",
    "test_url = \"https://atariame.bandcamp.com/album/voiceless\"\n",
    "test_html = urllib.request.urlopen(test_url) # Open the page\n",
    "artist_soup = BeautifulSoup(test_html,'html.parser')\n",
    "\n",
    "def get_artist_info(artist_soup):\n",
    "    n = 0\n",
    "    # Get Artist Name\n",
    "    artist_name = artist_soup.find(\"span\",{\"itemprop\":\"byArtist\"}).text\n",
    "    artist_name.replace(\"\\\\n\",\" \")\n",
    "    #print(artist_name)\n",
    "        \n",
    "    # Get Album Name\n",
    "    album_name = artist_soup.find(\"h2\",{\"class\":\"trackTitle\"}).text\n",
    "    album_name.replace(\"\\r\\n\",\" \")\n",
    "    #print(album_name)\n",
    "    \n",
    "    # Get Artist Location\n",
    "    artist_loc = artist_soup.find(\"span\",{\"class\":\"location secondaryText\"}).text\n",
    "    #print(artist_loc)\n",
    "    \n",
    "    # Get the date\n",
    "    date = artist_soup.find(\"meta\",{\"itemprop\":\"datePublished\"})[\"content\"]\n",
    "    #print(date)\n",
    "    \n",
    "    # Get Album Tags\n",
    "    for tags in artist_soup.find_all(\"a\",{\"class\":\"tag\"}): \n",
    "        TAGS_.append(tags.text)\n",
    "    #print(TAGS_)\n",
    "        \n",
    "    # Get Songs\n",
    "    for song in artist_soup.find_all(\"span\", {\"itemprop\":\"name\"}):\n",
    "        SONG_.append(song.text)       \n",
    "    #print(SONG_)\n",
    "    \n",
    "    for n_classes in artist_soup.find_all(\"div\",{\"class\":\"title\"}):\n",
    "        n = n + 1       \n",
    "        time_soup = artist_soup.find_all(\"span\",{\"class\":\"time secondaryText\"})   \n",
    "        print(len(time_soup))\n",
    "\n",
    "    print(n)\n",
    "    \n",
    "    # Get Song Lengths\n",
    "    for length in artist_soup.find_all(\"span\",{\"class\":\"time secondaryText\"}):\n",
    "        print(length)\n",
    "        \n",
    "        #if length.text == '':\n",
    "            #LENGTH_.append(\"N/A\")\n",
    "        #else:\n",
    "            #LENGTH_.append(length.text)\n",
    "    \n",
    "            \n",
    "        \n",
    "    \n",
    "    output = [artist_name,album_name,artist_loc,date,TAGS_,SONG_]\n",
    "    \n",
    "    return output\n",
    "    \n",
    "    \n",
    "output = get_artist_info(artist_soup)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ambient', 'idm', 'avant-pop', 'dark folk', 'rhythmic noise', 'shoegaze', 'Moscow']\n"
     ]
    }
   ],
   "source": [
    "TAGS_=[]\n",
    "for tags in artist_soup.find_all(\"a\",{\"class\":\"tag\"}): \n",
    "    TAGS_.append(tags.text)\n",
    "print(TAGS_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Test123.csv','w') as csvfile:\n",
    "    fieldnames = ['Tags']\n",
    "    writer = csv.DictWriter(csvfile,fieldnames=fieldnames,lineterminator = '\\n')\n",
    "    writer.writeheader()\n",
    "    myData = writer.writerow({'Tags':TAGS_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moscow, Russia\n"
     ]
    }
   ],
   "source": [
    "artist_loc = artist_soup.find(\"span\",{\"class\":\"location secondaryText\"})\n",
    "print(artist_loc.text)\n",
    "\n",
    "date = artist_soup.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_pages(soup):\n",
    "    n_pagees = []\n",
    "    for page in soup.find_all(\"a\",{\"rel\":\"nofollow\"}):\n",
    "        print(page.text)\n",
    "        n_pagees.append(int(page.text))\n",
    "    max_page = max(n_pagees)\n",
    "    return max_page\n",
    "\n",
    "n_pages = get_max_pages(soup)\n",
    "print(n_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poop = 20\n",
    "for x in range(0,poop):\n",
    "    print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/track/i-dont-feat-k\n",
      "just a track\n",
      "/track/red\n",
      "just a track\n",
      "/album/xxyyxx\n",
      "/track/pay-attention\n",
      "just a track\n",
      "/track/tinashe-let-you-love-me-remix\n",
      "just a track\n",
      "/album/mystify\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_url = \"https://xxyyxx.bandcamp.com/\"\n",
    "test_html = urllib.request.urlopen(test_url) # Open the page\n",
    "artist_soup = BeautifulSoup(test_html,'html.parser')\n",
    "\n",
    "try:\n",
    "# See if they have albums!\n",
    "    album_soup = artist_soup.find('ol', {\"data-edit-callback\" : \"/music_reorder\"})\n",
    "    for l in album_soup.find_all('a',href=True):\n",
    "        print(l[\"href\"])\n",
    "        if \"track\" in l[\"href\"]:\n",
    "            print(\"just a track\")\n",
    "                                   \n",
    "except: \n",
    "    print(\"only 1 album\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            I Don't (feat. $K)\n",
      "            \n",
      "        \n",
      "Los Angeles, California\n",
      "20171006\n",
      "['electronic', 'pop', 'r&b', 'Los Angeles']\n",
      "\n",
      "            Red\n",
      "            \n",
      "        \n",
      "Los Angeles, California\n",
      "20151110\n",
      "['electronic', 'red', 'Los Angeles']\n",
      "it's an album\n",
      "\n",
      "            Pay Attention\n",
      "            \n",
      "        \n",
      "Los Angeles, California\n",
      "20130611\n",
      "['electronic', 'experimental', 'xxyyxx', 'Los Angeles']\n",
      "\n",
      "            Tinashe - Let You Love Me (Remix)\n",
      "            \n",
      "        \n",
      "Los Angeles, California\n",
      "20120717\n",
      "['electronic', 'remix', 'sony', 'tinashe', 'xxyyxx', 'Los Angeles']\n",
      "it's an album\n"
     ]
    }
   ],
   "source": [
    "test_url = \"https://xxyyxx.bandcamp.com\"\n",
    "test_html = urllib.request.urlopen(test_url) # Open the page\n",
    "artist_soup = BeautifulSoup(test_html,'html.parser')\n",
    "related_tags = []\n",
    "try:\n",
    "# See if they have albums!\n",
    "    album_soup = artist_soup.find('ol', {\"data-edit-callback\" : \"/music_reorder\"})\n",
    "    for l in album_soup.find_all('a',href=True):\n",
    "        if \"track\" in l[\"href\"]:\n",
    "            track_url = test_url + l[\"href\"]\n",
    "            track_html = urllib.request.urlopen(track_url)\n",
    "            track_soup = BeautifulSoup(track_html,'html.parser')\n",
    "            song_title = track_soup.find(\"h2\",{\"class\":\"trackTitle\"}).text\n",
    "            print(song_title)\n",
    "            song_duration = track_soup.find(\"div\",{\"class\":'track_info'})           \n",
    "            location = track_soup.find(\"span\",{\"class\":\"location secondaryText\"}).text\n",
    "            print(location)\n",
    "            date = track_soup.find(\"meta\",{\"itemprop\":\"datePublished\"})[\"content\"]\n",
    "            print(date)\n",
    "            for tags in track_soup.find_all(\"a\",{\"class\":\"tag\"}): \n",
    "                related_tags.append(tags.text)\n",
    "            print(related_tags)\n",
    "            related_tags = []\n",
    "            \n",
    "        elif \"album\" in l[\"href\"]:\n",
    "            print(\"it's an album\")\n",
    "except: \n",
    "    print(\"OH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nice\n"
     ]
    }
   ],
   "source": [
    "test_url = \"https://karisxyz.bandcamp.com/track/00000\"\n",
    "test_html = urllib.request.urlopen(test_url) # Open the page\n",
    "artist_soup = BeautifulSoup(test_html,'html.parser')\n",
    "\n",
    "try:\n",
    "    artist_soup.select('div[class= \"inline_player one-track hidden\"]')\n",
    "    print('nice')\n",
    "except:\n",
    "    print(\"we out here\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    x = artist_soup.find_all('div',{\"class\":\"inline_player one-track hidden\"}).text\n",
    "    print('good')#Tells you if it's hidden/n/A\n",
    "except:\n",
    "    print('bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-145-3157c8ca4a4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0martist_soup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"class\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"inline_player one-track hidden\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"aria-label\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "x = artist_soup.find_all(\"div\",{\"class\":\"inline_player one-track hidden\"})\n",
    "\n",
    "print(x[\"aria-label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
