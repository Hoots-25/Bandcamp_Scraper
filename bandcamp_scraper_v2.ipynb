{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://bugreport.bandcamp.com\n",
      "3\n",
      "https://bambamtheband.bandcamp.com\n",
      "3\n",
      "https://djfukboi.bandcamp.com\n",
      "3\n",
      "https://vortexology.bandcamp.com\n",
      "2\n",
      "2\n",
      "2\n",
      "https://djsewell.bandcamp.com\n",
      "3\n",
      "https://devy.bandcamp.com\n",
      "1\n",
      "2\n",
      "https://dinsha.bandcamp.com\n",
      "3\n",
      "https://liquidnoise.bandcamp.com\n",
      "3\n",
      "https://severedeyes.bandcamp.com\n",
      "3\n",
      "https://eisenwolf.bandcamp.com\n",
      "3\n",
      "https://binges.bandcamp.com\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "https://alicespages.bandcamp.com\n",
      "2\n",
      "2\n",
      "https://l3u53ff3.bandcamp.com\n",
      "3\n",
      "https://tyro.bandcamp.com\n",
      "3\n",
      "https://blue-baggins.bandcamp.com\n",
      "3\n",
      "https://dicksmalone.bandcamp.com\n",
      "3\n",
      "https://timgorgeous.bandcamp.com\n",
      "3\n",
      "https://thequaintandthecurious.bandcamp.com\n",
      "3\n",
      "https://soyflip.bandcamp.com\n",
      "3\n",
      "https://jazzrev.bandcamp.com\n",
      "3\n",
      "https://michaelnothing.bandcamp.com\n",
      "3\n",
      "https://frisbeesinthefog.bandcamp.com\n",
      "3\n",
      "https://name10.bandcamp.com\n",
      "3\n",
      "https://fatalbreed.bandcamp.com\n",
      "3\n",
      "https://ohsuburbia.bandcamp.com\n",
      "3\n",
      "https://naalia.bandcamp.com\n",
      "3\n",
      "https://grace-en-espanol.bandcamp.com\n",
      "3\n",
      "https://goodflavortapes.bandcamp.com\n",
      "3\n",
      "https://calilamaire.bandcamp.com\n",
      "1\n",
      "1\n",
      "1\n",
      "https://nightingtale.bandcamp.com\n",
      "3\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-d4188b33d84e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    265\u001b[0m                     \u001b[1;31m# if it's an album\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m                     \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0martist_url\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/releases\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m                     \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m                     \u001b[0mcontent_soup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    529\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m             response = self.parent.error(\n\u001b[1;32m--> 641\u001b[1;33m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 569\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "# Open artist index\n",
    "# Count how many pages there are\n",
    "# Open the artist page\n",
    "# 1. Check if there are music/artist tabs\n",
    "    # 1.1 If there are artist tabs ignore them (they probably exist elsewhere and will get picked up)\n",
    "    # 1.2 If there are music tabs ignore them (they probably exist elsewhere and will get picked up)\n",
    "# 2. Check if there are multiple songs/tracks on the artist page\n",
    "    # 2.1 If it is a track, open the page, get info and write to csv (A single track will not contain a song duration)\n",
    "    # 2.2 if it is an album, open the page, get info and write to csv\n",
    "# 3. Check if all the artist page is also the album page (The artist page will only contain tracks and nothing else)\n",
    "    # 3.1 Get album information, and write it to a csv\n",
    "# 4. Something bad most likely happened and you should skip and go to the next artist\n",
    "\n",
    "##########################################################################################################################\n",
    "##########################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################################################\n",
    "# Define Functions to be Used\n",
    "##########################################################################################################################\n",
    "# GET_MAX_PAGES(soup)\n",
    "# Get the maximum page value to use as the end point (This is probably a waste of a function since you only need it 1x)\n",
    "\n",
    "def get_max_pages(soup):\n",
    "    n_pages = []\n",
    "    for page in soup.find_all(\"a\",{\"rel\":\"nofollow\"}):\n",
    "        n_pages.append(int(page.text))\n",
    "    max_pages = max(n_pages)\n",
    "    return max_pages\n",
    "\n",
    "\n",
    "##########################################################################################################################\n",
    "# GET_TAB_CHECK()\n",
    "# This function will check if there are multiple tabs on the artist page. \n",
    "# A return value of 0: There is at least one tab\n",
    "# A return value of 1 mean there are no tabs\n",
    "\n",
    "def get_tab_check(asrtist_soup):    \n",
    "    check_1 = artist_soup.find(\"a\",{\"href\":\"/artists\"})\n",
    "    check_2 = artist_soup.find(\"a\",{\"href\":\"/music\"})\n",
    "    \n",
    "    if check_1 is None or check_2 is None:\n",
    "        # If the there is no music OR no Artist Tab\n",
    "        tab_check = 0\n",
    "        \n",
    "    else:\n",
    "        # Tabs exist \n",
    "        tab_check = 1\n",
    "        \n",
    "    return tab_check\n",
    "\n",
    "##########################################################################################################################\n",
    "# GET_SINGLE_ALBUM_INFO(soup)\n",
    "# This function should be called when on an artists page and they only have a single album (multiple tracks)\n",
    "\n",
    "def get_single_album_data(content_soup):    \n",
    "    artist_song = []\n",
    "    song_dur = []\n",
    "    artist_tags = []\n",
    "    \n",
    "    # Get Artist Name\n",
    "    artist_name = content_soup.find(\"span\", {\"itemprop\":\"byArtist\"}).text\n",
    "    artist_name = artist_name.replace(\"\\n\",\"\")\n",
    "    artist_name.lstrip().rstrip()\n",
    "    #print(artist_name)\n",
    "            \n",
    "    # Get Album Name\n",
    "    album_name = content_soup.find(\"h2\", {\"class\":\"trackTitle\"}).text\n",
    "    album_name = album_name.replace(\"\\n\",\"\")\n",
    "        \n",
    "    # Get Artist Location\n",
    "    artist_loc = content_soup.find(\"span\", {\"class\":\"location secondaryText\"}).text\n",
    "    if artist_loc == \"\":\n",
    "        artist_loc = \"N/A\"\n",
    "    \n",
    "    # Get the date\n",
    "    try:\n",
    "        album_date = content_soup.find(\"meta\", {\"itemprop\":\"datePublished\"})[\"content\"]\n",
    "        album_date = datetime.strptime(album_date, \"%Y%m%d\").strftime(\"%m/%d/%Y\")\n",
    "    except:\n",
    "        album_date = \"N/A\"\n",
    "    \n",
    "    # Get Album Tags\n",
    "    for tags in content_soup.find_all(\"a\", {\"class\":\"tag\"}): \n",
    "        artist_tags.append(tags.text)\n",
    "            \n",
    "    # Get Songs\n",
    "    try:\n",
    "        content_soup.find_all(\"span\", {\"itemprop\":\"name\"})\n",
    "        for song in content_soup.find_all(\"span\", {\"itemprop\":\"name\"}):\n",
    "            artist_song.append(song.text)\n",
    "        # artist_song = [artist_song.replace(\" \", \"\") for artist_song in song]\n",
    "        # artist_song = [artist_song.replace(\"\\n\", \"\") for artist_song in song]\n",
    "    except:\n",
    "        artist_song = album_name\n",
    "        \n",
    "    # Get Song Durations\n",
    "    for x in content_soup.find_all(\"div\", {\"class\":\"title\"}):\n",
    "        try:\n",
    "            y = x.find(\"span\", {\"class\":\"time secondaryText\"})\n",
    "            song_dur.append(y.text)           \n",
    "        except:             \n",
    "            song_dur.append(\"N/A\")  # Pass N/A if song length is missing\n",
    "            \n",
    "    data = [artist_name, album_name, artist_loc, album_date, artist_tags, artist_song, song_dur]     \n",
    "    return data\n",
    "\n",
    "##########################################################################################################################\n",
    "# GET_SINGLE_TRACK_DATA(soup)\n",
    "# This function should be called when on an artists page and they only have a single track instead of an album as a page\n",
    "\n",
    "def get_single_track_data(content_soup):\n",
    "    artist_song = []\n",
    "    song_dur = []\n",
    "    artist_tags = []\n",
    "    \n",
    "    # Get Artist Name\n",
    "    artist_name = content_soup.find(\"span\", {\"itemprop\":\"byArtist\"}).text\n",
    "    artist_name = artist_name.replace(\"\\n\",\"\")\n",
    "    artist_name = artist_name.lstrip().rstrip()\n",
    "    \n",
    "    #print(artist_name)\n",
    "            \n",
    "    # Get Album Name\n",
    "    album_name = content_soup.find(\"h2\", {\"class\":\"trackTitle\"}).text\n",
    "    album_name = album_name.replace(\"\\n\",\"\")\n",
    "        \n",
    "    # Get Artist Location\n",
    "    artist_loc = content_soup.find(\"span\", {\"class\":\"location secondaryText\"}).text\n",
    "    if artist_loc == \"\":\n",
    "        artist_loc = \"N/A\"\n",
    "        \n",
    "    # Get the date\n",
    "    try:\n",
    "        album_date = content_soup.find(\"meta\", {\"itemprop\":\"datePublished\"})[\"content\"]\n",
    "        album_date = datetime.strptime(album_date, \"%Y%m%d\").strftime(\"%m/%d/%Y\")\n",
    "    except:\n",
    "        album_date = \"N/A\"\n",
    "    \n",
    "    # Get Album Tags\n",
    "    for tags in content_soup.find_all(\"a\", {\"class\":\"tag\"}): \n",
    "        artist_tags.append(tags.text) \n",
    "            \n",
    "    # Get Song \n",
    "    artist_song = album_name # Keep the track name the same as the album since\n",
    "    \n",
    "    # Get Song Durations\n",
    "    song_dur = \"N/A\" # Unable to find the duration of the song via beautifulSoup for some reason, so ignore (FIX THIS)\n",
    "            \n",
    "    data = [artist_name, album_name, artist_loc, album_date, artist_tags, artist_song, song_dur]     \n",
    "    return data\n",
    "\n",
    "##########################################################################################################################\n",
    "# WRITE_TO_CSV(data)\n",
    "# Self-explanatory\n",
    "\n",
    "def write_to_csv(data):\n",
    "    \n",
    "    # Pull out the data from the function\n",
    "    ARTIST_ = data[0]\n",
    "    ALBUM_ = data[1]\n",
    "    LOC_ = data[2]\n",
    "    DATE_ = data[3]\n",
    "    TAGS_ = data[4]\n",
    "    SONG_ = data[5]\n",
    "    DUR_ = data[6]    \n",
    "    \n",
    "    song_array = np.array(SONG_)\n",
    "    n_songs = song_array.size   \n",
    "    \n",
    "    if n_songs > 1:\n",
    "        for i in range(0,n_songs): \n",
    "            #myData = writer.writerow({'Artist':ARTIST_,'Album':ALBUM_,'Song':SONG_[i],'Length':DUR_[i],'Tags':TAGS_,'Date':DATE_,'Location':LOC_})\n",
    "            writer.writerow({'Artist':ARTIST_,'Album':ALBUM_,'Song':SONG_[i],'Length':DUR_[i],'Tags':TAGS_,'Date':DATE_,'Location':LOC_})\n",
    "    elif n_songs == 1:\n",
    "            myData = writer.writerow({'Artist':ARTIST_,'Album':str(ALBUM_),'Song':str(SONG_),'Length':DUR_,'Tags':TAGS_,'Date':DATE_,'Location':LOC_})\n",
    "    else:\n",
    "        pass\n",
    "    return\n",
    "\n",
    "##########################################################################################################################\n",
    "##########################################################################################################################\n",
    "\n",
    "# Import all the necessary packages\n",
    "import urllib.request, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import csv \n",
    "import datetime\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "##########################################################################################################################\n",
    "\n",
    "# Start the main code \n",
    "artist_index_url = \"https://bandcamp.com/artist_index\" # Set the url for all artists\n",
    "html = urllib.request.urlopen(artist_index_url) # Open the page\n",
    "soup = BeautifulSoup(html,'html.parser') # Use beautifulSoup to access html     \n",
    "\n",
    "max_pages = get_max_pages(soup) # Find the maximum # pages so you know how long to run the for loop\n",
    "currentDT = datetime.datetime.now()\n",
    "currentDT = currentDT.strftime(\"%Y-%m-%d_%H.%M.%S\")\n",
    "\n",
    "# Create/Open the CSV File you will be writing too\n",
    "with open('Bandcamp_MASTER_' + str(currentDT) + '.csv','w',encoding='utf-8-sig') as csvfile:\n",
    "    fieldnames = ['Artist','Album','Location','Song','Length','Tags','Date']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames = fieldnames, lineterminator = '\\n')\n",
    "    writer.writeheader()\n",
    "    \n",
    "    page_url = \"?page=1\" # Initialize the 1st page as default  \n",
    "    \n",
    "    # This will loop for every page available (1-->max_pages)\n",
    "    for page in range(1, max_pages):\n",
    "        try:\n",
    "            # Get the url for the page number and open it with BeautifulSoup\n",
    "            page_url = page_url.replace(page_url[len(page_url) - 1], str(page)) # replace the previous page number    \n",
    "            page_html = artist_index_url + page_url # create the new link for the next page to scan\n",
    "            page_html = urllib.request.urlopen(page_html) # open the next page\n",
    "            soup = BeautifulSoup(page_html,'html.parser') # Pull the data from the next page         \n",
    "        \n",
    "            # Get the artists on that page and append it to the complete artist list\n",
    "\n",
    "            artist_class = soup.find('ul', {\"class\":\"item_list\"}) # Finds the html section that includes all artist links\n",
    "            for a_link in artist_class.find_all('a', href = True):\n",
    "                # print(a_link[\"href\"]) # print the artist's url\n",
    "                artist_url = a_link[\"href\"] # create variable for artist's url\n",
    "                artist_html = urllib.request.urlopen(artist_url) # open the artist's url\n",
    "                print(artist_url)\n",
    "                artist_soup = BeautifulSoup(artist_html,\"html.parser\") # Pull the data from artist's url     \n",
    "                                   \n",
    "                tab_check = get_tab_check(artist_soup)\n",
    "            \n",
    "                if tab_check == 0:\n",
    "                    # If there are no tabs\n",
    "                \n",
    "                    post_check = artist_soup.find(\"ol\", {\"data-edit-callback\" : \"/music_reorder\"}) # looks for a grid with multiple albums/tracks\n",
    "                \n",
    "                    if post_check is not None:\n",
    "                        # Confirms that there are multiple posts                    \n",
    "                        for n_posts in post_check.find_all(\"a\", href = True):          \n",
    "                            # Enter the artist page and identify the artist's content\n",
    "                            url = artist_url + n_posts[\"href\"]\n",
    "                            html = urllib.request.urlopen(url)\n",
    "                            content_soup = BeautifulSoup(html,\"html.parser\")                        \n",
    "                                        \n",
    "                            if \"track\" in n_posts[\"href\"]:\n",
    "                                # The soup is a track\n",
    "                                print(1)\n",
    "                                #print(url)\n",
    "                                data = get_single_track_data(content_soup)\n",
    "                                write_to_csv(data)\n",
    "                    \n",
    "                            elif \"album\" in n_posts[\"href\"]:\n",
    "                                # the soup is an album\n",
    "                                print(2)\n",
    "                                data = get_single_album_data(content_soup)\n",
    "                                write_to_csv(data)\n",
    "                            else: \n",
    "                                # Skip to the next artist if it was neither\n",
    "                                continue\n",
    "                    else:\n",
    "                        # There is one post, so open up their releases\n",
    "                        print(3)\n",
    "                        # if it's an album\n",
    "                        url = artist_url + \"/releases\"                  \n",
    "                        html = urllib.request.urlopen(url)\n",
    "                        content_soup = BeautifulSoup(html,\"html.parser\") \n",
    "                    \n",
    "                        # check if there are multiple tracks\n",
    "                        post_check2 = artist_soup.find(\"table\", {\"class\" : \"track_list track_table\"})\n",
    "                    \n",
    "                        if post_check2 is not None:\n",
    "                            # There are multiple tracks\n",
    "                            data = get_single_album_data(content_soup)\n",
    "                            write_to_csv(data)\n",
    "                        else:                        \n",
    "                            data = get_single_track_data(content_soup)\n",
    "                            write_to_csv(data)         \n",
    "                    \n",
    "                else:\n",
    "                    # If there are tabs, skip and go to the next artist\n",
    "                    continue\n",
    "        except:\n",
    "            continue\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
